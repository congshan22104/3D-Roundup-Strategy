# Centralized RL 实现

## 算法特点
- 集中式训练和执行
- 将多智能体系统视为单一控制问题
- 考虑所有智能体的全局状态和联合动作

## 核心组件
- `CentralizedRL.py`: 集中式学习算法的主要实现
- `DDPG_agent.py`: 改进的 DDPG 算法
- `NN_actor.py`: 集中式 Actor 网络
- `NN_critic.py`: 集中式 Critic 网络

## 优缺点
优点：
- 能获得理论上的最优策略
- 完整利用全局信息

缺点：
- 状态空间和动作空间随智能体数量指数增长
- 实际部署时需要集中式控制


| 2025.2.18 updated.
<br>
1. 共享reward函数。
2. 定义：所有智能体共享同一个全局网络，通过智能体ID或角色标识区分。<br>
输入输出：<br>
Actor：接收自身观测+智能体ID，输出动作。(待核实)<br>  
Critic：接收全局状态+所有动作+智能体ID，输出Q值。（待核实）<br>